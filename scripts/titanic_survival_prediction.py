# -*- coding: utf-8 -*-
"""Titanic Survival Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vvz1zpBHMujnheJ4bg6t6nzBv6Q41VK2

##Dependency Imports
"""

import numpy as np # For numerical operations
import pandas as pd # For data manipulation and analysis
import matplotlib.pyplot as plt # For creating static, interactive, and animated visualizations
import seaborn as sns # For making statistical graphics based on matplotlib
from sklearn.model_selection import train_test_split # For splitting data into training and testing sets
from sklearn.linear_model import LogisticRegression # For implementing the Logistic Regression model
from sklearn.metrics import accuracy_score # For evaluating the accuracy of the model
titanic_data = pd.read_csv('train.csv')
print(titanic_data.head())

"""## Missing Value Handling"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# This code block is for handling missing values in the dataset.
# Check for the sum of null values in each column to understand data completeness.
titanic_data.isnull().sum() #sum of null values

# Drop the 'Cabin' column as it has a large number of missing values and might not be crucial for the model.
if 'Cabin' in titanic_data.columns:
    titanic_data = titanic_data.drop(columns='Cabin', axis=1)

# Replace the missing values in the 'Age' column with the mean age to maintain data integrity and prevent data loss.
titanic_data['Age'] = titanic_data['Age'].fillna(titanic_data['Age'].mean())

# Find the mode value for the 'Embarked' column.
embarked_mode = titanic_data['Embarked'].mode()
# Replace the missing values in the 'Embarked' column with its mode value.
# This handles cases where the mode might be empty (e.g., if Embarked column is all NaNs), using a default 'S'.
if not embarked_mode.empty:
    titanic_data['Embarked'] = titanic_data['Embarked'].fillna(embarked_mode[0])
else:
    titanic_data['Embarked'] = titanic_data['Embarked'].fillna('S')

print(titanic_data['Embarked'].mode())

"""## Data Transformation



"""

# Convert categorical string values into numerical representations for model compatibility.
# 'Sex' is mapped: 'male' to 0, 'female' to 1.
# 'Embarked' is mapped: 'S' (Southampton) to 0, 'C' (Cherbourg) to 1, 'Q' (Queenstown) to 2.
titanic_data.replace({'Sex':{'male':0,'female':1}, 'Embarked':{'S':0,'C':1,'Q':2}}, inplace=True)

# Define a list of columns to drop. These columns are considered irrelevant for the prediction model
# because they are unique identifiers ('PassengerId', 'Ticket') or text fields ('Name') that do not
# directly contribute to the survival prediction in this model.
columns_to_drop = ['PassengerId','Name','Ticket']

# Check which columns actually exist in the DataFrame before attempting to drop them
# This ensures the code is robust against cases where a column might already have been dropped or renamed.
existing_columns_to_drop = [col for col in columns_to_drop if col in titanic_data.columns]

# Drop the identified irrelevant columns from the DataFrame.
if existing_columns_to_drop:
    titanic_data = titanic_data.drop(columns=existing_columns_to_drop, axis=1)

# Convert categorical string values into numerical representations for model compatibility.
# 'Sex' is mapped: 'male' to 0, 'female' to 1.
# 'Embarked' is mapped: 'S' (Southampton) to 0, 'C' (Cherbourg) to 1, 'Q' (Queenstown) to 2.
titanic_data = titanic_data.replace({'Sex':{'male':0,'female':1}, 'Embarked':{'S':0,'C':1,'Q':2}})

# Define a list of columns to drop. These columns are considered irrelevant for the prediction model
# because they are unique identifiers ('PassengerId', 'Ticket') or text fields ('Name') that do not
# directly contribute to the survival prediction in this model.
columns_to_drop = ['PassengerId','Name','Ticket']

# Check which columns actually exist in the DataFrame before attempting to drop them
# This ensures the code is robust against cases where a column might already have been dropped or renamed.
existing_columns_to_drop = [col for col in columns_to_drop if col in titanic_data.columns]

# Drop the identified irrelevant columns from the DataFrame.
if existing_columns_to_drop:
    titanic_data = titanic_data.drop(columns=existing_columns_to_drop, axis=1)

"""## Structure Data Splitting

"""

# Separate features (X) and target variable (Y).
# X contains all columns except 'Survived', which are the features used to predict survival.
X = titanic_data.drop(columns = ['Survived'], axis = 1)
# Y contains only the 'Survived' column, which is the target variable we want to predict.
Y = titanic_data['Survived']

# Split the dataset into training and testing sets.
# X_train and Y_train will be used to train the model.
# X_test and Y_test will be used to evaluate the model's performance.
# test_size=0.2 means 20% of the data will be used for the test set, and 80% for the training set.
# random_state=2 ensures reproducibility; the same split will be generated every time the code is run.
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)

# Separate features (X) and target variable (Y).
# X contains all columns except 'Survived', which are the features used to predict survival.
X = titanic_data.drop(columns = ['Survived'], axis = 1)
# Y contains only the 'Survived' column, which is the target variable we want to predict.
Y = titanic_data['Survived']

# Split the dataset into training and testing sets.
# X_train and Y_train will be used to train the model.
# X_test and Y_test will be used to evaluate the model's performance.
# test_size=0.2 means 20% of the data will be used for the test set, and 80% for the training set.
# random_state=2 ensures reproducibility; the same split will be generated every time the code is run.
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)

"""## Model Training and Evaluation"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Initialize the Logistic Regression model.
# Logistic Regression is a statistical model that in its basic form uses a logistic function
# to model a binary dependent variable.
model = LogisticRegression()

# Train the model with the training data.
# The 'fit' method trains the model using the features (X_train) to learn the patterns
# that predict the target variable (Y_train).
model.fit(X_train, Y_train)

# Make predictions on the training data.
# This helps to evaluate how well the model learned the training patterns.
X_train_prediction = model.predict(X_train)

# Calculate the accuracy score for the training data.
# accuracy_score compares the true labels (Y_train) with the predicted labels (X_train_prediction).
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)
print('Accuracy score of training data : ', training_data_accuracy)

# Make predictions on the test data.
# This is crucial for evaluating the model's generalization performance on unseen data.
X_test_prediction = model.predict(X_test)

# Calculate the accuracy score for the test data.
# This score indicates how well the model performs on new, unseen data, which is a better
# indicator of its real-world performance than training accuracy.
test_data_accuracy = accuracy_score(Y_test, X_test_prediction)
print('Accuracy score of test data : ', test_data_accuracy)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Initialize the Logistic Regression model.
# Logistic Regression is a statistical model that in its basic form uses a logistic function
# to model a binary dependent variable.
# Increased max_iter to 1000 to help with convergence, as suggested by the warning.
model = LogisticRegression(max_iter=1000)

# Train the model with the training data.
# The 'fit' method trains the model using the features (X_train) to learn the patterns
# that predict the target variable (Y_train).
model.fit(X_train, Y_train)

# Make predictions on the training data.
# This helps to evaluate how well the model learned the training patterns.
X_train_prediction = model.predict(X_train)

# Calculate the accuracy score for the training data.
# accuracy_score compares the true labels (Y_train) with the predicted labels (X_train_prediction).
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)
print('Accuracy score of training data : ', training_data_accuracy)

# Make predictions on the test data.
# This is crucial for evaluating the model's generalization performance on unseen data.
X_test_prediction = model.predict(X_test)

# Calculate the accuracy score for the test data.
# This score indicates how well the model performs on new, unseen data, which is a better
# indicator of its real-world performance than training accuracy.
test_data_accuracy = accuracy_score(Y_test, X_test_prediction)
print('Accuracy score of test data : ', test_data_accuracy)

"""## Prediction Logic"""

# Example input data for making a prediction.
# This tuple represents a hypothetical passenger's features:
# (Pclass, Sex, Age, SibSp, Parch, Fare, Embarked)
# For example, (3, 0, 35, 0, 0, 8.05, 0) means Pclass=3, Sex=male(0), Age=35, SibSp=0, Parch=0, Fare=8.05, Embarked=S(0).
input_data = (3,0,35,0,0,8.05,0)

# Convert the input data tuple to a NumPy array.
# This is necessary because the model expects numerical array input.
input_data_as_numpy_array = np.asarray(input_data)

# Reshape the NumPy array to indicate that we are predicting for a single instance.
# The model expects a 2D array where each row is a sample and columns are features.
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# Make a prediction using the trained Logistic Regression model.
# The predict method returns an array of predictions (0 for dead, 1 for alive).
prediction = model.predict(input_data_reshaped)

# Interpret the prediction and print the result.
# If the prediction is 0, the model predicts the passenger did not survive.
if prediction[0] == 0:
    print('Dead')
# If the prediction is 1, the model predicts the passenger survived.
if prediction[0] == 1:
    print('Alive')

# Example input data for making a prediction.
# This tuple represents a hypothetical passenger's features:
# (Pclass, Sex, Age, SibSp, Parch, Fare, Embarked)
# For example, (3, 0, 35, 0, 0, 8.05, 0) means Pclass=3, Sex=male(0), Age=35, SibSp=0, Parch=0, Fare=8.05, Embarked=S(0).
input_data = (3,0,35,0,0,8.05,0)

# Convert the input data tuple to a NumPy array.
# This is necessary because the model expects numerical array input.
input_data_as_numpy_array = np.asarray(input_data)

# Reshape the NumPy array to indicate that we are predicting for a single instance.
# The model expects a 2D array where each row is a sample and columns are features.
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# Convert the reshaped input data into a DataFrame with column names matching X_train.
# This prevents a UserWarning about missing feature names during prediction.
input_df = pd.DataFrame(input_data_reshaped, columns=X_train.columns)

# Make a prediction using the trained Logistic Regression model.
# The predict method returns an array of predictions (0 for dead, 1 for alive).
prediction = model.predict(input_df)

# Interpret the prediction and print the result.
# If the prediction is 0, the model predicts the passenger did not survive.
if prediction[0] == 0:
    print('Dead')
# If the prediction is 1, the model predicts the passenger survived.
if prediction[0] == 1:
    print('Alive')

"""## Summary:

### Data Analysis Key Findings

*   **Data Cleaning and Preprocessing:**
    *   The 'Cabin' column was dropped due to a large number of missing values.
    *   Missing 'Age' values were imputed using the mean age.
    *   Missing 'Embarked' values were filled with the mode ('S').
    *   Categorical features 'Sex' and 'Embarked' were transformed into numerical representations: 'male' to 0, 'female' to 1 for 'Sex'; 'S' to 0, 'C' to 1, 'Q' to 2 for 'Embarked'.
    *   Irrelevant columns such as 'PassengerId', 'Name', and 'Ticket' were dropped to prevent them from influencing the model.
*   **Data Splitting:**
    *   The dataset was divided into features (X) and target (Y, 'Survived').
    *   The data was split into training (80%) and testing (20%) sets using `train_test_split` with `random_state=2` to ensure reproducibility.
*   **Model Training and Evaluation:**
    *   A Logistic Regression model was initialized with `max_iter=1000` to ensure convergence.
    *   The model achieved a training accuracy of approximately 80.9% and a test accuracy of about 78.2%.
*   **Prediction Logic:**
    *   New input data for prediction is prepared by converting it into a NumPy array, reshaping it for single-instance prediction, and then transforming it into a Pandas DataFrame with column names matching the training features.
    *   An example prediction for a hypothetical passenger `(Pclass=3, Sex=male(0), Age=35, SibSp=0, Parch=0, Fare=8.05, Embarked=S(0))` resulted in a prediction of "Dead."
*   **Code Robustness:**
    *   A `FutureWarning` related to `inplace=True` in `replace` was resolved by reassigning the DataFrame.
    *   A `ConvergenceWarning` from `LogisticRegression` was addressed by increasing `max_iter` to 1000.
    *   A `UserWarning` regarding missing feature names during prediction was resolved by passing input data as a DataFrame with correct column names.

### Insights or Next Steps

*   The Logistic Regression model shows a reasonable performance with a test accuracy of ~78.2%, suggesting it generalizes fairly well to unseen data, although there's a slight drop from training accuracy, which is common.
*   To potentially improve model performance, further steps could include exploring more advanced feature engineering, trying different machine learning algorithms, or performing hyperparameter tuning on the Logistic Regression model.

"""